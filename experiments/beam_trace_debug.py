from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import torch.nn.functional as F


model_id = "facebook/opt-1.3b"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True).to(device)

#prompt = "The future of AI is"
prompt = "The future of AI is expected to be"
inputs = tokenizer(prompt, return_tensors="pt").to(device)

outputs = model.generate(
    **inputs,
    num_beams=4,
    return_dict_in_generate=True,
    output_scores=True,
    max_new_tokens=50,
)


sequence = outputs.sequences[0]  # æœ€çµ‚å‡ºåŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ï¼ˆBeam 0 ã®ã¿ã¨ã¯é™ã‚‰ãªã„ï¼‰
token_ids = sequence[inputs["input_ids"].shape[-1]:]  # ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿

print(f"Total tokens in sequence: {len(token_ids)}")
print(f"Available scores:         {len(outputs.scores)}")
print(f"Available beam_indices:   {len(outputs.beam_indices)}")



logprobs = []
prev_beam = 0  # åˆæœŸãƒ“ãƒ¼ãƒ ï¼ˆä»®å®šï¼‰

print("\nğŸ“Š Beam Trace per Token with Beam Switching Info:")
for step, (token_id, score_logits, beam_idx) in enumerate(zip(token_ids, outputs.scores, outputs.beam_indices)):
    probs = F.log_softmax(score_logits, dim=-1)
    token_logprobs = probs[:, token_id.item()]  # [num_beams]

    # æœ€ã‚‚å°¤ã‚‚ã‚‰ã—ã„ãƒ“ãƒ¼ãƒ ï¼ˆã“ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å‡ºåŠ›ã—ãŸã¨æ€ã‚ã‚Œã‚‹ãƒ“ãƒ¼ãƒ ï¼‰
    current_beam = torch.argmax(token_logprobs).item()
    logprob = token_logprobs[current_beam].item()
    token_str = tokenizer.decode([token_id])

    if current_beam != prev_beam:
        switch_info = f"â†’ switched from Beam {prev_beam}"
    else:
        switch_info = ""

    print(f"Step {step+1:2}: Token: {token_str:15} | LogProb: {logprob:7.2f} | From Beam {current_beam} {switch_info}")
    prev_beam = current_beam