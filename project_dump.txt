# ğŸ“ Project Directory Tree
```
.
â”œâ”€â”€ README.md
â”œâ”€â”€ REPORT.md
â”œâ”€â”€ config
â”‚Â Â  â””â”€â”€ default.yaml
â”œâ”€â”€ decoding
â”‚Â Â  â”œâ”€â”€ beam_search.py
â”‚Â Â  â””â”€â”€ speculative.py
â”œâ”€â”€ experiments
â”‚Â Â  â””â”€â”€ run.py
â”œâ”€â”€ export_project.sh
â”œâ”€â”€ main.py
â”œâ”€â”€ models
â”‚Â Â  â”œâ”€â”€ load_models.py
â”‚Â Â  â””â”€â”€ prompt_renderer.py
â”œâ”€â”€ project_dump.txt
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ setup.sh
â””â”€â”€ uv.lock

5 directories, 14 files
```

# ğŸ“„ .py ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§

## â–¶ï¸ ./decoding/beam_search.py
```py
# decoding/beam_search.py
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

def beam_generate(model, tokenizer, input_ids, device, num_beams=5, max_tokens=50):
    with torch.no_grad():
        output = model.generate(
            input_ids.to(device),
            num_beams=num_beams,
            max_new_tokens=max_tokens,
            early_stopping=True,
            pad_token_id=tokenizer.eos_token_id
        )
    return tokenizer.decode(output[0], skip_special_tokens=True)
```

## â–¶ï¸ ./decoding/speculative.py
```py
import torch
from transformers import LogitsProcessorList, MinLengthLogitsProcessor
from models.prompt_renderer import render_prompt

def speculative_generate(draft_model, target_model, tokenizer, input_ids, device, k=4, max_tokens=50):
    generated = input_ids.clone()

    while generated.shape[1] < max_tokens:
        with torch.no_grad():
            draft_outputs = draft_model.generate(
                generated,
                max_new_tokens=k,
                do_sample=True,
                top_k=50,
                temperature=0.7,
                pad_token_id=tokenizer.eos_token_id
            )

        draft_tokens = draft_outputs[:, generated.shape[1]:]
        accepted_tokens = []

        for i in range(draft_tokens.shape[1]):
            current_input = torch.cat([generated, draft_tokens[:, :i]], dim=-1)
            with torch.no_grad():
                target_outputs = target_model(current_input)
                next_token_logits = target_outputs.logits[:, -1, :]
                predicted_token = torch.argmax(next_token_logits, dim=-1)

            if predicted_token.item() == draft_tokens[:, i].item():
                accepted_tokens.append(draft_tokens[:, i])
            else:
                with torch.no_grad():
                    fallback = target_model.generate(
                        current_input,
                        max_new_tokens=1,
                        pad_token_id=tokenizer.eos_token_id
                    )
                generated = fallback
                break
        else:
            if accepted_tokens:
                accepted = torch.cat(accepted_tokens, dim=-1).unsqueeze(0)
                generated = torch.cat([generated, accepted], dim=-1)

    return tokenizer.decode(generated[0], skip_special_tokens=True)

```

## â–¶ï¸ ./experiments/run.py
```py
import sys, os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

import hydra
from omegaconf import DictConfig, OmegaConf
import wandb
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

from models.load_models import load_model_and_tokenizer
from models.prompt_renderer import render_prompt  # âœ… è¿½åŠ 
from decoding.speculative import speculative_generate
from decoding.beam_search import beam_generate

@hydra.main(version_base=None, config_path="../config", config_name="default")
def main(cfg: DictConfig):
    print("\n=== Experiment Config ===")
    print(OmegaConf.to_yaml(cfg))

    wandb.init(
        project=cfg.project_name,
        config=OmegaConf.to_container(cfg, resolve=True)
    )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"[INFO] Using device: {device}")

    draft_model, draft_tokenizer = load_model_and_tokenizer(cfg.draft_model, device)
    target_model, target_tokenizer = load_model_and_tokenizer(cfg.target_model, device)
    print(f"[INFO] Draft model: {cfg.draft_model}")
    print(f"[INFO] Target model: {cfg.target_model}")

    for i, prompt in enumerate(cfg.prompts):
        print(f"\n[Prompt {i+1}] {prompt}")

        # âœ… ã“ã“ã§ input_ids ã‚’æº–å‚™ï¼ˆChatTemplateå¯¾å¿œï¼‰
        inputs = render_prompt(
            prompt,
            tokenizer=draft_tokenizer if cfg.decode.mode == "speculative" else target_tokenizer,
            use_system_prompt=cfg.chat.use_system_prompt,
            system_prompt=cfg.chat.system_prompt,
            debug=cfg.debug.render_input_text
        )
        input_ids = inputs.input_ids.to(device)

        if cfg.decode.mode == "speculative":
            output = speculative_generate(
                draft_model, target_model, draft_tokenizer,
                input_ids, device,
                k=cfg.decode.k,
                max_tokens=cfg.decode.max_tokens
            )
        else:
            output = beam_generate(
                target_model, target_tokenizer,
                input_ids, device,
                max_tokens=cfg.decode.max_tokens
            )

        print("\n=== Output ===")
        print(output)
        wandb.log({f"prompt_{i+1}": prompt, f"output_{i+1}": output})

if __name__ == "__main__":
    main()
```

## â–¶ï¸ ./main.py
```py
def main():
    print("Hello from speculative-beam-decoding!")


if __name__ == "__main__":
    main()
```

## â–¶ï¸ ./models/load_models.py
```py
# models/load_models.py
from transformers import AutoTokenizer, AutoModelForCausalLM

def load_model_and_tokenizer(model_name, device):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)  # â† GPUå¯¾å¿œ
    model.eval()
    return model, tokenizer
```

## â–¶ï¸ ./models/prompt_renderer.py
```py
from transformers import BatchEncoding
import torch

def render_prompt(prompt, tokenizer, use_system_prompt=False, system_prompt=None, debug=False):
    can_use_chat_template = (
        hasattr(tokenizer, "chat_template")
        and tokenizer.chat_template is not None
    )

    if can_use_chat_template:
        messages = []
        if use_system_prompt and system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        if debug:
            rendered = tokenizer.apply_chat_template(messages, tokenize=False)
            print("\n[DEBUG] Input rendered by ChatTemplate:\n" + rendered)

        result = tokenizer.apply_chat_template(messages, return_tensors="pt")
        # âœ… æˆ»ã‚Šå€¤ãŒ Tensor ã®å ´åˆã€è‡ªå‹•ã§ãƒ©ãƒƒãƒ—ã™ã‚‹
        if isinstance(result, torch.Tensor):
            return BatchEncoding({"input_ids": result})
        return result

    else:
        if debug:
            print("\n[DEBUG] Raw prompt input:\n" + prompt)
        return tokenizer(prompt, return_tensors="pt")
```

# ğŸ“„ .md ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§

## â–¶ï¸ ./README.md
```md
# ğŸš€ Speculative Beam Decoding: Combining Speed and Quality in Language Generation

This repository explores the combination of **Speculative Decoding** and **Beam Search** for efficient and high-quality language generation using Transformer-based language models.

---

## ğŸ“Œ Motivation

While **Speculative Decoding** accelerates generation by leveraging a fast draft model and validating outputs with a slower, more accurate target model, **Beam Search** improves output quality through breadth in candidate exploration.

This project investigates:
- How speculative decoding can **bootstrap multiple beams** efficiently
- How beam search can **refine rejected completions** from speculative drafts
- The trade-offs in **speed vs. coherence/quality** when combining both

---

## ğŸ§  Conceptual Overview

```mermaid
graph TD
    A[Input Prompt] --> B{Draft Model}
    B --> C[Drafted Beams (Top-k)]
    C --> D{Verified by Target Model}
    D -->|Accept| E[Keep Beam]
    D -->|Reject| F{Beam Search Correction}
    F --> G[Final Output]
    E --> G
```

- **Drafting**: Fast model (e.g., DistilGPT2) proposes multiple beams.
- **Verification**: Target model (e.g., GPT2 or GPT-J) checks validity.
- **Fallback**: Beam search is invoked on rejected branches to recover quality.

---

## ğŸ”§ Project Structure

```
speculative-beam-decoding/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ config/
â”‚   â””â”€â”€ default.yaml
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ load_models.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ decoding/
â”‚   â”œâ”€â”€ speculative.py
â”‚   â””â”€â”€ beam_search.py
â”œâ”€â”€ experiments/
â”‚   â””â”€â”€ run_ablation.py
â”œâ”€â”€ analysis/
â”‚   â””â”€â”€ eval_metrics.py
â””â”€â”€ notebooks/
    â””â”€â”€ exploratory.ipynb
```

---

## ğŸš€ Getting Started

### 1. Clone & Install Dependencies

```bash
git clone https://github.com/your-username/speculative-beam-decoding.git
cd speculative-beam-decoding
pip install -r requirements.txt
```

### 2. Run a Sample

```bash
python experiments/run_ablation.py --prompt "The future of AI is"
```

---

## ğŸ“Š Evaluation

Metrics used:
- **Latency**: Tokens/sec
- **Perplexity** (on verified output)
- **BLEU / ROUGE** (on downstream summarization)
- **Beam diversity**

Compare:
- Greedy decoding
- Beam search
- Speculative decoding
- Speculative + beam hybrid

---

## ğŸ—ï¸ Planned Features

- [ ] Multi-beam speculative validation
- [ ] Token-level fallback and correction
- [ ] GPU batch-mode speculative beam
- [ ] Web demo (Streamlit or Gradio)
- [ ] Integration with LLaMA2 or GPT-NeoX

---

## ğŸ§ª Sample Output

> **Prompt**: "In the year 2050, humans and machines will"

âœ… *Draft model*: "In the year 2050, humans and machines will coexist in harmony and share resources for the betterment"

âœ… *Target model verified*: âœ“âœ“âœ“âœ“âœ“âœ“âœ—âœ—

ğŸ” *Beam search fallback*: "cooperate on sustainable projects"

ğŸ§¾ *Final*: "In the year 2050, humans and machines will cooperate on sustainable projects."

---

## ğŸ“š References

- OpenAI, "Accelerating LLMs with Speculative Decoding" (2023)
- Google, "Beam Search Strategies for Neural Text Generation"
- Hugging Face `transformers` library

---

## ğŸ§‘â€ğŸ’» Author

Created by [Your Name](https://github.com/your-username).  
Feel free to open issues, discussions, or PRs!

```

## â–¶ï¸ ./REPORT.md
```md
# ğŸ§ª Experiment Report: Speculative Decoding + Beam Search with ChatTemplate Models

## 1. å®Ÿè£…å†…å®¹

æœ¬å®Ÿé¨“ã¯ã€ä»¥ä¸‹ã®è¦ç´ ã‚’çµ±åˆçš„ã«çµ„ã¿è¾¼ã‚“ã **æ¨è«–åŠ¹ç‡å‘ä¸Šã®ãŸã‚ã®æ¯”è¼ƒå®Ÿé¨“åŸºç›¤**ã§ã‚ã‚‹ï¼š

- ğŸ¤– **Speculative Decoding** ã®å®Ÿè£…ï¼ˆè‰æ¡ˆãƒ¢ãƒ‡ãƒ« + æœ¬ç•ªãƒ¢ãƒ‡ãƒ«ã®å”èª¿ç”Ÿæˆï¼‰
- ğŸ¯ **Beam Search** ã®å®Ÿè£…ï¼ˆå˜ä¸€ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹å¹…æ¢ç´¢å‹ãƒ‡ã‚³ãƒ¼ãƒ‰ï¼‰
- ğŸ”§ **Hydra + W&B** ã«ã‚ˆã‚‹æŸ”è»Ÿãªè¨­å®šç®¡ç†ãƒ»å®Ÿé¨“ãƒ­ã‚°ä¿å­˜
- ğŸ§© **ChatTemplate å¯¾å¿œ**ï¼š`tokenizer.apply_chat_template()` ã«ã‚ˆã‚‹çµ±ä¸€ã•ã‚ŒãŸãƒãƒ£ãƒƒãƒˆå…¥åŠ›æ§‹æˆ
- âš™ï¸ `render_prompt()` é–¢æ•°ã«ã‚ˆã‚Š CausalLM / Chat å‹ã‚’çµ±åˆçš„ã«å‡¦ç†
- CLI ã§åˆ‡æ›¿å¯èƒ½ãª `system_prompt`, `debug.render_input_text` ã«ã‚ˆã‚‹å…¥åŠ›å¯è¦–åŒ–

---

## 2. å®Ÿè£…ã®å¤‰é·

### âœ… åˆæœŸæ§‹æˆ
- `distilgpt2` / `gpt2` ã‚’ä½¿ç”¨ã—ãŸ **Speculative Decoding** ã®ãƒˆã‚¤å®Ÿè£…
- `prompt` ã‚’ã‚³ãƒ¼ãƒ‰å†…ã§ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
- `tokenizer(prompt)` ã«ã‚ˆã‚‹ãƒ—ãƒ¬ãƒ¼ãƒ³ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
- è­¦å‘Šï¼ˆ`attention_mask not set`ï¼‰ãŒå‡ºã‚‹ã‚‚ç„¡è¦–

### ğŸ”„ æ®µéšçš„é€²åŒ–
1. `prompt` ã‚’ `config/default.yaml` ã«ç§»å‹• â†’ CLI ã‹ã‚‰å·®ã—æ›¿ãˆå¯èƒ½ã«
2. `prompts: [ ... ]` ã«ã‚ˆã‚‹ **è¤‡æ•°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒƒãƒè©•ä¾¡** ã«å¯¾å¿œ
3. `render_prompt()` ã®å°å…¥ã«ã‚ˆã‚Š **ChatTemplate ãƒ¢ãƒ‡ãƒ«ã®çµ±åˆ**
4. `debug.render_input_text` ã«ã‚ˆã‚‹ **å…¥åŠ›å¯è¦–åŒ–** ã®ãƒˆã‚°ãƒ«å®Ÿè£…
5. `input_ids` ã‚’ run.py å´ã§çµ±ä¸€ç”Ÿæˆã—ã€`speculative_generate` / `beam_generate` ã«æ¸¡ã™æ–¹å¼ã¸ç§»è¡Œ
6. `tokenizer.chat_template is not None` ã«ã‚ˆã‚‹å®‰å…¨åˆ¤å®šå°å…¥
7. `TinyLlama-Chat` â†’ `Gemma 3` â†’ `Qwen2.5-3B-Instruct` ã¸ã®ãƒ¢ãƒ‡ãƒ«é€²åŒ–

---

## 3. æ°—ã¥ã„ãŸã“ã¨ãƒ»å­¦ã³

- âœ… ChatTemplate ã¯ HuggingFace ã§å¾ã€…ã«æ¨™æº–åŒ–ã•ã‚Œã¤ã¤ã‚ã‚‹ãŒã€è¿”ã‚Šå€¤ãŒ `Tensor` ã®ã¿ã®å ´åˆã‚‚ã‚ã‚Šæ³¨æ„ï¼ˆè¦ `BatchEncoding` ãƒ©ãƒƒãƒ—ï¼‰
- âš ï¸ `hasattr(tokenizer, "apply_chat_template")` ã ã‘ã§ã¯ä¸ååˆ†ã€‚`tokenizer.chat_template is not None` ã®ãƒã‚§ãƒƒã‚¯ãŒæœ¬è³ª
- âœ… `prompt_renderer` é–¢æ•°ã«ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚„ãƒ‡ãƒãƒƒã‚°æ©Ÿèƒ½ã‚’çµ±åˆã™ã‚‹ã“ã¨ã§ã€å®Ÿé¨“æ™‚ã®æŒ™å‹•ã‚’å³å¯è¦–åŒ–ã§ããŸ
- âœ… ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦ `system_prompt` ã®åŠ¹æœãŒæ˜ç¢ºã«ç•°ãªã‚‹ãŸã‚ã€CLIã‹ã‚‰ã®åˆ‡æ›¿ãŒéå¸¸ã«æœ‰ç”¨
- âœ… `TinyLlama` ã¯è»½é‡ã ãŒå†…å®¹ã¯æµ…ãã€`Zephyr`, `Gemma`, `Qwen` ãªã©ã® ChatTemplate ç³»ãƒ¢ãƒ‡ãƒ«ã¯ã‚ˆã‚Šè‡ªç„¶ãªå‡ºåŠ›ã‚’ç¤ºã—ãŸ
- âœ… `speculative decoding` ã¯ draft ãƒ¢ãƒ‡ãƒ«ã®å“è³ªã«å¤§ããä¾å­˜ã—ã€ä½å“è³ªã ã¨ target model ãŒè¿½ã„ã¤ã fallback é€£ç™ºã«ãªã‚Šã‚„ã™ã„
- âœ… `torch._dynamo.exc.Unsupported` ãªã©ã®å†…éƒ¨ã‚¨ãƒ©ãƒ¼ã¯ãƒ¢ãƒ‡ãƒ«ã® transformer å®Ÿè£…ãŒæ–°ã—ã™ãã‚‹å ´åˆã«èµ·ã“ã‚‹ï¼ˆGemma3ï¼‰

---

## 4. å®Ÿè£…ã—ãŸãƒ‡ã‚³ãƒ¼ãƒ‰æ‰‹æ³•ã®æ¦‚è¦

### ğŸ§  Speculative Decoding

è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ï¼ˆdraft + targetï¼‰ã‚’ç”¨ã„ã¦ã€é«˜é€Ÿæ€§ã¨å“è³ªã‚’ä¸¡ç«‹ã™ã‚‹æ–¹æ³•ï¼š

1. **è‰æ¡ˆãƒ¢ãƒ‡ãƒ«ï¼ˆdraftï¼‰** ãŒ `k` å€‹ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’äºˆæ¸¬ï¼ˆé«˜é€Ÿï¼‰
2. **æœ¬ç•ªãƒ¢ãƒ‡ãƒ«ï¼ˆtargetï¼‰** ãŒãã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é€æ¬¡æ¤œè¨¼
3. ä¸ä¸€è‡´ãŒã‚ã‚Œã° fallbackï¼ˆtarget ã«ã‚ˆã‚‹å†ç”Ÿæˆï¼‰
4. ä¸€è‡´ã—ãŸãƒˆãƒ¼ã‚¯ãƒ³ã¯ã¾ã¨ã‚ã¦æ¡ç”¨ã—é«˜é€ŸåŒ–

```text
[Prompt] â†’ [è‰æ¡ˆãƒ¢ãƒ‡ãƒ« â†’ kãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆ] â†’ [targetãƒ¢ãƒ‡ãƒ«ã§æ¤œè¨¼] â†’ ä¸€è‡´ï¼Ÿ â†’ YES â†’ æ¡ç”¨ / NO â†’ fallback
```

- `generate()` ã® `top_k`, `temperature` ã«ã‚ˆã‚Š draft ã®å¤šæ§˜æ€§ã‚’åˆ¶å¾¡
- é«˜å“è³ªãªè‰æ¡ˆã‚’å‡ºã›ã‚Œã°ã€ç”Ÿæˆé€Ÿåº¦ãŒæœ€å¤§5å€ä»¥ä¸Šã«å‘ä¸Šã™ã‚‹å ´åˆã‚‚

---

### ğŸ¯ Beam Search

å¾“æ¥ã®æ¢ç´¢å‹ç”Ÿæˆæ–¹å¼ï¼š

- äºˆæ¸¬ã®éš›ã«ã€Œæœ€ã‚‚ç¢ºã‹ã‚‰ã—ã„ `n` æœ¬ã®ãƒ“ãƒ¼ãƒ ï¼ˆæ–‡è„ˆï¼‰ã€ã‚’æ®‹ã—ã¤ã¤å±•é–‹
- æœ€çµ‚çš„ã«æœ€è‰¯ã®ãƒ“ãƒ¼ãƒ ã‚’é¸æŠ

```text
Step 1: ç¾åœ¨ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾ã— top-n å€™è£œã‚’å±•é–‹
Step 2: å„å€™è£œã«ã¤ã„ã¦å†å¸°çš„ã«æ¢ç´¢
Step 3: åˆè¨ˆç¢ºç‡ãŒæœ€å¤§ã®ç³»åˆ—ã‚’å‡ºåŠ›
```

- å¤šæ§˜æ€§ã«ä¹ã—ã„ãŒã€æ±ºå®šçš„ã§å®‰å®šã—ãŸç”Ÿæˆã«å¼·ã„
- `num_beams` ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒéµ

---

## ğŸ“ ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã¨æ‰€æ„Ÿ

| ãƒ¢ãƒ‡ãƒ« | ä½¿ç”¨å¯å¦ | ChatTemplate | æ‰€æ„Ÿ |
|--------|----------|--------------|------|
| distilgpt2 / gpt2 | âœ… | âŒ | ãƒˆã‚¤å®Ÿé¨“ã«ã¯è»½ãã¦è‰¯ã„ãŒå‡ºåŠ›ãŒå˜èª¿ |
| TinyLlama-Chat | âœ… | âœ… | è¶…è»½é‡ãƒ»å³å¿œç­”ã ãŒçŸ¥è­˜ã¯æµ…ã‚ |
| Gemma 3 4B | âŒï¼ˆã‚¨ãƒ©ãƒ¼ï¼‰ | âœ… | ChatTemplateã‚ã‚Šã ãŒ Transformers å´æœªå¯¾å¿œã‚ã‚Š |
| Zephyr 1.3B | âœ… | âœ… | é«˜å“è³ªãƒ»è»½é‡ã§å®Ÿç”¨å‘ã |
| Qwen2.5-3B-Instruct | âœ… | âœ… | å¤šè¨€èªï¼‹ChatTemplateå¯¾å¿œã€æ¨å¥¨ãƒ¢ãƒ‡ãƒ« |

---

ä»¥ä¸ŠãŒã€æœ¬å®Ÿé¨“ã§å¾—ã‚‰ã‚ŒãŸæ§‹æˆãƒ»çŸ¥è¦‹ã®ã¾ã¨ã‚ã§ã™ã€‚ä»Šå¾Œã¯ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã€system promptåˆ¶å¾¡ã€å‡ºåŠ›å“è³ªè©•ä¾¡ï¼ˆBLEU, perplexityï¼‰ãªã©ã«æ‹¡å¼µã§ãã¾ã™ã€‚```

# ğŸ“„ .yaml ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§

## â–¶ï¸ ./config/default.yaml
```yaml
project_name: specbeam-study

draft_model: Qwen/Qwen2.5-3B-Instruct
target_model: Qwen/Qwen2.5-3B-Instruct


chat:
  use_system_prompt: true
  system_prompt: "You are a concise and helpful assistant."

debug:
  render_input_text: true

decode:
  mode: speculative  # or "beam"
  k: 4
  max_tokens: 50

prompts:
  - "The future of AI is"
  - "In 2050, machines and humans will collaborate to"
  - "Explain the concept of speculative decoding"
  - "What is the meaning of consciousness?"
  - "Once upon a time in a digital world,"


```

